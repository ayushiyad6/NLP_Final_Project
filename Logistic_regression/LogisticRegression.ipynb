{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jlr_9n6lDKsh",
        "outputId": "1c90f441-2653-4452-877a-8d1bf67db661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  label\n",
            "0  I grew up (b. 1965) watching and loving the Th...      0\n",
            "1  When I put this movie in my DVD player, and sa...      0\n",
            "2  Why do people who do not know what a particula...      0\n",
            "3  Even though I have great interest in Biblical ...      0\n",
            "4  Im a die hard Dads Army fan and nothing will e...      1\n",
            "Accuracy is:  0.699\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Author: Ayushi Yadav \n",
        "        Nandan Prasad\n",
        "        Sai Pavan\n",
        "\n",
        "Title : Final_Project.py\n",
        "\n",
        "Description : The python module executes the logistic regression model.\n",
        "\n",
        "Functions: It has 5 functions:\n",
        "            1. clean_train_tweets(df)\n",
        "            2. clean_data()\n",
        "            3. vectorize()\n",
        "            4. test_review()\n",
        "            5. test_data()\n",
        "            \n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "import csv\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import re\n",
        "from stop_words import get_stop_words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet \n",
        "import nltk\n",
        "import nltk.corpus\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "from nltk.util import bigrams\n",
        "from nltk.util import everygrams\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet \n",
        "\n",
        "global tempArr\n",
        "tempArr = []\n",
        "\n",
        "global vectorizer\n",
        "global classifier\n",
        "\n",
        "replace_no_space = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\|)|(\\()|(\\))|(\\[)|(\\])|(/%)|(\\$)|(\\>)|(\\<)|(\\{)|(\\})\")\n",
        "replace_with_space = re.compile(\"(<br\\s/><br\\s/?)|(-)|(/)|(:).\")\n",
        "\n",
        "df_test = pd.read_csv(\"Test.csv\")\n",
        "\n",
        "df_train = pd.read_csv(\"clean_train.csv\")\n",
        "\n",
        "x_train = df_train.text.values\n",
        "y_train = df_train.label.values\n",
        "\n",
        "x_test = df_test.text.values\n",
        "y_test = df_test.label.values\n",
        "\n",
        "\n",
        "def clean_train_tweets(df):\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        # tmpl = preprocessor.clean(df[i])                                 # preprocessor library to clean data\n",
        "        tmpl = replace_no_space.sub(\"\", df[i].lower())        # lower casing\n",
        "        tmpl = replace_with_space.sub(\" \", tmpl)             # replacing with a space in case of special char\n",
        "        line = tmpl.lower()\n",
        "        stop_words = get_stop_words('english')               # removing stopwords\n",
        "        word_tokens = list(line.split())\n",
        "        # filtered_sentence = []                               # initialize empty list\n",
        "        # filtered_sentence.append('</s>')\n",
        "        filtered_sentence =\"\"\n",
        "        for w in word_tokens:\n",
        "            if w not in stop_words:\n",
        "                ps = nltk.LancasterStemmer()\n",
        "                # lm = nltk.WordNetLemmatizer()\n",
        "                wor = ps.stem(w)\n",
        "                filtered_sentence = filtered_sentence + wor + \" \"\n",
        "        global tempArr\n",
        "        tempArr.append(filtered_sentence)                   # appending it to gloabl array\n",
        "    return tempArr\n",
        "\n",
        "\n",
        "\n",
        "def clean_data():\n",
        "\n",
        "    df = pd.read_csv(\"Train.csv\")\n",
        "    print(df.head())\n",
        "    train_reviews = clean_train_tweets(df[\"text\"])\n",
        "    # creating file\n",
        "    headerList = ['text', 'label']\n",
        "    filename = \"clean_train.csv\"\n",
        "    with open(filename, 'w') as file:\n",
        "        dw = csv.DictWriter(file, delimiter=',', \n",
        "                        fieldnames=headerList)\n",
        "        dw.writeheader()\n",
        "    for i in range(len(tempArr)):\n",
        "        x = random.randint(0,1000000)\n",
        "        data_to_append = ({ 'text' : [tempArr[i]], 'score' : [df.label[i]]})\n",
        "        df2 = pd.DataFrame(data_to_append)\n",
        " \n",
        "        # append data frame to CSV file\n",
        "        df2.to_csv(filename, mode='a', index=False, header=False)\n",
        "\n",
        "\n",
        "\n",
        "def vectorize():\n",
        "\n",
        "# vectorize\n",
        "    global vectorizer\n",
        "    vectorizer = CountVectorizer()\n",
        "    vectorizer.fit(x_train)\n",
        "    X_train = vectorizer.transform(x_train)\n",
        "    X_test = vectorizer.transform(x_test)\n",
        "\n",
        "    global classifier\n",
        "    classifier = LogisticRegression(max_iter = 1000)\n",
        "    classifier.fit(X_train, y_train)\n",
        "\n",
        "    score = classifier.score(X_test, y_test)\n",
        "    print (\"Accuracy is: \", score)\n",
        "\n",
        "    y_pred = classifier.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred, labels= df_train.label.unique())\n",
        "    df_cm = pd.DataFrame(cm, index = df_train.label.unique(), columns = df_train.label.unique())\n",
        "\n",
        "def test_review():\n",
        "    tweet = \"I am good\"\n",
        "    global vectorizer\n",
        "    global classifier\n",
        "    vectTweet = vectorizer.transform(np.array([tweet]))\n",
        "    prediction = classifier.predict(vectTweet)\n",
        "\n",
        "\n",
        "def test_data():\n",
        "#testing the data\n",
        "    global classifier\n",
        "    global vectorizer\n",
        "    tempArr.clear()\n",
        "    clean_test_df = clean_train_tweets(df_test['text'])\n",
        "    Test_score = []\n",
        "    for i in clean_test_df:\n",
        "        sc = vectorizer.transform(np.array([i]))\n",
        "        prediction = classifier.predict(sc)\n",
        "        Test_score.append(prediction[0])\n",
        "\n",
        "\n",
        "    headerList = ['text', 'label']\n",
        "    filename = \"test_result.csv\"\n",
        "    with open(filename, 'w') as file:\n",
        "            dw = csv.DictWriter(file, delimiter=',', \n",
        "                            fieldnames=headerList)\n",
        "            dw.writeheader()\n",
        "    for i in range(len(Test_score)):\n",
        "        x = random.randint(0,1000000)\n",
        "        data_to_append = ({ 'text' : [df_test[\"text\"][i]], 'score' : [Test_score[i]]})\n",
        "        df2 = pd.DataFrame(data_to_append)\n",
        " \n",
        "        # append data frame to CSV file\n",
        "        df2.to_csv(filename, mode='a', index=False, header=False)\n",
        "\n",
        "\n",
        "clean_data()\n",
        "vectorize()\n",
        "test_review()\n",
        "test_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJbmepPPDLkH",
        "outputId": "6d53d0bc-04d5-4b04-8041-a72ec132d54c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stop_words\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: stop_words\n",
            "  Building wheel for stop_words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop_words: filename=stop_words-2018.7.23-py3-none-any.whl size=32910 sha256=7f6c0a8181b2ab852cc219210010b1af8c92b72c742213e312bd602dbf7fcbdc\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/1a/23/f12552a50cb09bcc1694a5ebb6c2cd5f2a0311de2b8c3d9a89\n",
            "Successfully built stop_words\n",
            "Installing collected packages: stop_words\n",
            "Successfully installed stop_words-2018.7.23\n"
          ]
        }
      ]
    }
  ]
}