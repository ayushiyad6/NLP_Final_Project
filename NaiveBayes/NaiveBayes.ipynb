{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jlr_9n6lDKsh",
        "outputId": "da805b48-5863-4b89-a0b7-1e756d21a2c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  label\n",
            "0  I grew up (b. 1965) watching and loving the Th...      0\n",
            "1  When I put this movie in my DVD player, and sa...      0\n",
            "2  Why do people who do not know what a particula...      0\n",
            "3  Even though I have great interest in Biblical ...      0\n",
            "4  Im a die hard Dads Army fan and nothing will e...      1\n",
            "The model score is:  0.8118\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Author: Ayushi Yadav \n",
        "        Nandan Prasad\n",
        "        Sai Pavan\n",
        "\n",
        "Date: 30th March 2023\n",
        "\n",
        "Title : NLP_Project_NB.py\n",
        "\n",
        "Description : The python code trains the dataset of Naive Bayes model, \n",
        "            tests the data and gives the score of the model that is achieved.\n",
        "\n",
        "Functions: It has 5 functions:\n",
        "            1. Create_Test_Dataset(TestFile, TestFilelabel)\n",
        "                The test dataset contains 0, 1 and -1 labels. \n",
        "                Filtering the data with only 0 and 1 labels.\n",
        "            2. clean_train_tweets(df)\n",
        "                Cleaning the data\n",
        "            3. clean_data()\n",
        "                saves the clean data in a file\n",
        "            4. train_NB_model(path_to_train_file)\n",
        "                training the model with the clean train data\n",
        "            5. test_NB_model(path_to_test_file, NB_model)\n",
        "                testing the test data on file created in step 1.\n",
        "            6. Model_score(path_to_test_file, NB_model)\n",
        "                Gives the score of the model.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import csv\n",
        "from stop_words import get_stop_words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet \n",
        "import nltk\n",
        "import nltk.corpus\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "from nltk.util import bigrams\n",
        "from nltk.util import everygrams\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "global tempArr\n",
        "tempArr = []\n",
        "\n",
        "global vec\n",
        "\n",
        "replace_no_space = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\|)|(\\()|(\\))|(\\[)|(\\])|(/%)|(\\$)|(\\>)|(\\<)|(\\{)|(\\})\")\n",
        "replace_with_space = re.compile(\"(<br\\s/><br\\s/?)|(-)|(/)|(:).\")\n",
        "\n",
        "\n",
        "\n",
        "def clean_train_tweets(df):\n",
        "\n",
        "    # clearing the tempArr list if previous entries exist\n",
        "\n",
        "    global tempArr\n",
        "    tempArr.clear()\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        tmpl = df[i]                               # preprocessor library to clean data\n",
        "        if '\\n' in tmpl:                                    # Any new lines present in the same column remove it\n",
        "            tmpl = tmpl.replace('\\n', \" \")\n",
        "        if r'<br />' in tmpl:\n",
        "            tmpl = tmpl.replace(r'<br />', \" \")\n",
        "        # tmpl = replace_no_space.sub(\"\", tmpl.lower())        # lower casing\n",
        "        tmpl = replace_with_space.sub(\" \", tmpl)             # replacing with a space in case of special char\n",
        "        line = tmpl.lower()\n",
        "        stop_words = get_stop_words('english')               # removing stopwords\n",
        "        word_tokens = list(line.split())\n",
        "        # filtered_sentence = []                               # initialize empty list\n",
        "        # filtered_sentence.append('</s>')\n",
        "        filtered_sentence =\"\"\n",
        "        for w in word_tokens:\n",
        "            if w not in stop_words:\n",
        "                ps = nltk.LancasterStemmer()\n",
        "                # lm = nltk.WordNetLemmatizer()\n",
        "                wor = ps.stem(w)\n",
        "                filtered_sentence = filtered_sentence + wor + \" \"\n",
        "        tempArr.append(filtered_sentence)                   # appending it to gloabl array\n",
        "    return tempArr\n",
        "\n",
        "\n",
        "\n",
        "def clean_data():\n",
        "\n",
        "    # Creating the file and the header of the csv file.\n",
        "\n",
        "    df = pd.read_csv(\"Train.csv\")\n",
        "    print(df.head())\n",
        "    train_reviews = clean_train_tweets(df[\"text\"])\n",
        "\n",
        "    # creating file\n",
        "    headerList = ['text', 'label']\n",
        "    filename = \"clean_train.csv\"\n",
        "    with open(filename, 'w') as file:\n",
        "        dw = csv.DictWriter(file, delimiter=',', \n",
        "                        fieldnames=headerList)\n",
        "        dw.writeheader()\n",
        "    \n",
        "    # Populating the csv file\n",
        "\n",
        "    for i in range(len(tempArr)):\n",
        "        x = random.randint(0,1000000)\n",
        "        data_to_append = ({ 'text' : [tempArr[i]], 'label' : [df.label[i]]})\n",
        "        df2 = pd.DataFrame(data_to_append)\n",
        " \n",
        "        # append data frame to CSV file\n",
        "        df2.to_csv(filename, mode='a', index=False, header=False)\n",
        "\n",
        "\n",
        "\n",
        "def train_NB_model(path_to_train_file):\n",
        "    \n",
        "    # segregating the data x_train and y_train_model\n",
        "\n",
        "    df_train = pd.read_csv(path_to_train_file)\n",
        "    x_train = df_train.text.fillna(' ')\n",
        "    y_train_label = df_train['label']\n",
        "\n",
        "    # creating vectorizer object\n",
        "\n",
        "    vectorizer = CountVectorizer()\n",
        "    x_train_count = vectorizer.fit_transform(x_train)\n",
        "\n",
        "    global vec\n",
        "    vec = vectorizer\n",
        "\n",
        "    # Training the model\n",
        "\n",
        "    model = MultinomialNB()\n",
        "    model.fit(x_train_count, y_train_label)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test_NB_model(path_to_test_file, NB_model):\n",
        "\n",
        "    # Reading the testing file path\n",
        "\n",
        "    test = pd.read_csv(path_to_test_file)\n",
        "    testArray = clean_train_tweets(test[\"text\"])\n",
        "\n",
        "    # creating an empty list of score of each and every entry\n",
        "    testArrayscore = []\n",
        "\n",
        "    # vectorizer = CountVectorizer()\n",
        "\n",
        "    # populating the list\n",
        "    for item in testArray:\n",
        "        cmt = vec.transform(np.array([item]))\n",
        "        result = NB_model.predict(cmt)\n",
        "        testArrayscore.append(result[0])\n",
        "\n",
        "    # creating file\n",
        "    headerList = ['text', 'label']\n",
        "    filename = \"final_result.csv\"\n",
        "    df = pd.read_csv(\"Train.csv\")\n",
        "    with open(filename, 'w') as file:\n",
        "        dw = csv.DictWriter(file, delimiter=',', \n",
        "                        fieldnames=headerList)\n",
        "        dw.writeheader()\n",
        "    df = pd.read_csv(\"Test.csv\")\n",
        "\n",
        "    # Populating the csv file with the tweet and the score\n",
        "\n",
        "    for i in range(len(testArrayscore)):\n",
        "        data_to_append = ({ 'text' : [df[\"text\"][i]], 'label' : [testArrayscore[i]]})\n",
        "        df2 = pd.DataFrame(data_to_append)\n",
        " \n",
        "        # append data frame to CSV file\n",
        "        df2.to_csv(filename, mode='a', index=False, header=False)\n",
        "\n",
        "\n",
        "def Model_score(path_to_test_file, NB_model):\n",
        "\n",
        "    # vectorizer = CountVectorizer()\n",
        "    test = pd.read_csv(path_to_test_file)\n",
        "    x_test = test.text\n",
        "    y_test = test.label\n",
        "    x_test_count = vec.transform(x_test)\n",
        "    model_score = NB_model.score(x_test_count, y_test)\n",
        "    print(\"The model score is: \", model_score)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Calling the various functions\n",
        "\n",
        "clean_data()\n",
        "\n",
        "NB_model = train_NB_model(\"clean_train.csv\")\n",
        "\n",
        "test_NB_model(\"Test.csv\", NB_model)\n",
        "\n",
        "Model_score(\"Test.csv\", NB_model)\n",
        "\n",
        "# Code ends"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJbmepPPDLkH",
        "outputId": "6d53d0bc-04d5-4b04-8041-a72ec132d54c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stop_words\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: stop_words\n",
            "  Building wheel for stop_words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop_words: filename=stop_words-2018.7.23-py3-none-any.whl size=32910 sha256=7f6c0a8181b2ab852cc219210010b1af8c92b72c742213e312bd602dbf7fcbdc\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/1a/23/f12552a50cb09bcc1694a5ebb6c2cd5f2a0311de2b8c3d9a89\n",
            "Successfully built stop_words\n",
            "Installing collected packages: stop_words\n",
            "Successfully installed stop_words-2018.7.23\n"
          ]
        }
      ]
    }
  ]
}