{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3H28qk71vu_",
        "outputId": "47aa16b7-f336-4a22-a745-adfe0bd6be09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[16908  3111]\n",
            " [ 3355 16626]]\n",
            "F1-Score :  0.8372022760461251\n",
            "Accuracy :  83.83500000000001\n"
          ]
        }
      ],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def pre_process(dataset):\n",
        "    global stop_words\n",
        "    pattern = RegexpTokenizer(r'\\w+')\n",
        "    pattern2 = re.compile('[0-9]+')\n",
        "\n",
        "    new_list = []\n",
        "\n",
        "    for i in range(0,len(dataset)):\n",
        "        temp = dataset[i]\n",
        "        if '\\n' in temp:  # Any new lines present in the same column remove it\n",
        "            temp = temp.replace('\\n', \" \")\n",
        "        if r'<br />' in temp:\n",
        "            temp = temp.replace(r'<br />', \" \")\n",
        "        temp_word_list =  pattern.tokenize(temp)\n",
        "        s = \"\"\n",
        "        temp_set = set()\n",
        "        result = []\n",
        "        for j in temp_word_list:\n",
        "            j = j.lower()\n",
        "            if re.search(pattern2, j):\n",
        "                pass\n",
        "            else:\n",
        "                if j not in stop_words:\n",
        "                    if j not in temp_set:\n",
        "                        j= stemmer.stem(j)\n",
        "                        temp_set.add(j)\n",
        "                        result.append((j))\n",
        "\n",
        "        # print(result)\n",
        "        for k in result:\n",
        "            s = s + \" \" + k\n",
        "        new_list.append(word_tokenize(s))\n",
        "\n",
        "    return new_list\n",
        "\n",
        "train_set =pd.read_csv(r'/content/Test.csv')\n",
        "test_set =pd.read_csv(r'/content/Train.csv')\n",
        "train_set['new_text'] = pre_process(train_set['text'])\n",
        "test_set['new_text'] = pre_process(test_set['text'])\n",
        "\n",
        "train = train_set['new_text'].apply(lambda x: ' '.join(x))\n",
        "train_label = train_set['label']\n",
        "test = test_set['new_text'].apply(lambda x: ' '.join(x))\n",
        "test_label = test_set['label']\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "count_vect = CountVectorizer(stop_words='english')\n",
        "transformer = TfidfTransformer(norm='l2',sublinear_tf=True)\n",
        "\n",
        "x_train_counts = count_vect.fit_transform(train)\n",
        "x_train_tfidf = transformer.fit_transform(x_train_counts)\n",
        "#print(x_train_counts.shape)\n",
        "#print(x_train_tfidf.shape)\n",
        "\n",
        "#Output :(25569, 27304) (25569, 27304)\n",
        "x_test_counts = count_vect.transform(test)\n",
        "x_test_tfidf = transformer.transform(x_test_counts)\n",
        "#print(x_test_counts.shape)\n",
        "#print(x_test_tfidf.shape)\n",
        "#Output : (6393, 27304) (6393, 27304)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators=200)\n",
        "model.fit(x_train_tfidf,train_label)\n",
        "predictions = model.predict(x_test_tfidf)\n",
        "\n",
        "#Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix,f1_score\n",
        "CM = confusion_matrix(test_label,predictions)\n",
        "print(CM)\n",
        "\n",
        "#f1-score\n",
        "F1 = f1_score(test_label,predictions)\n",
        "print(\"F1-Score : \", F1)\n",
        "\n",
        "\n",
        "#Accuracy_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "Acc = accuracy_score(test_label,predictions)*100\n",
        "print(\"Accuracy : \", Acc)\n",
        "\n"
      ]
    }
  ]
}